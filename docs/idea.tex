\documentclass{article}

\title{Replacement Language Modeling}
\author{Bill Ray}
\date{\today}

\begin{document}

\maketitle

\section{Setup}

\subsection{Empty Token}
A token sequence can be decoded into a string by concatenating the characters of the tokens. The empty token is a special token that represents the empty string. Inserting the empty token into a token sequence at any point does not change the string that is decoded from the sequence. Token sequences that decode to the same string are functionally equivalent. Consider the empty token to be part of the vocabulary going forward.

\subsection{Replacement Process}
Start with a sequence of tokens where every non-empty token is neighbored by empty tokens on both sides. Let's call this format the "normal form" of a sequence. Let the empty token ID in the vocabulary be $0$. Let all other tokens IDs correspond to non-empty tokens.

\vspace*{1em}
\noindent Example Sequence of token IDs: $[0, 23, 0, 14, 0, 18, 0]$
\vspace*{1em}

Replace zero or more tokens in the sequences with other tokens from the vocabulary. Notice that replacing a non-empty token with the empty token effectively deletes that token and replacing an empty token with a non-empty token inserts that token into the string that is decoded from the sequence. While replacement can change the length of the string that is decoded from the token sequence, the length of the token sequence is not changed during replacement.

\vspace*{1em}
\noindent Example Replacement: $[0, 23, 0, 14, 0, 18, 0] \rightarrow [0, 85, 54, 14, 0, 0, 0]$
\vspace*{1em}

After replacement, restore the sequence to normal form by replacing any consecutive runs of empty tokens with a single empty token and insert an empty token between any two non-empty tokens.

\vspace*{1em}
\noindent Example Normalization: $[0, 85, 54, 14, 0, 0, 0] \rightarrow [0, 85, 0, 54, 0, 14, 0]$
\vspace*{1em}

\subsection{Levenshtein edit distance}
The Levenshtein distance is defined between two sequences as the minimum number of edits (insertions, deletions, and substitutions) required to change one sequence into another.

For example, the character-level Levenshtein distance between "kitten" and "sitting" is 3, since the following 3 edits change one into the other, and there is no way to do it with fewer than 3 edits:
\begin{enumerate}
    \item{kitten $\rightarrow$ sitten (substitution of "s" for "k")}
    \item{sitten $\rightarrow$ sittin (substitution of "i" for "e")}
    \item{sittin $\rightarrow$ sitting (insertion of "g" at the end)}
\end{enumerate}

\subsection{Insertions, deletions, and substitutions as replacements}
Given a sequence in normal form:

$[0, 23, 0, 14, 0, 18, 0]$

An insertion can be made by replacing an empty token with a non-empty token:

$[0, 23, 0, 14, 0, 18, 0] \rightarrow [0, 23, 54, 14, 0, 18, 0] \rightarrow [0, 23, 0, 54, 0, 14, 0, 18, 0]$

A deletion can be made by replacing a non-empty token with an empty token:

$[0, 23, 0, 14, 0, 18, 0] \rightarrow [0, 23, 0, 0, 0, 18, 0] \rightarrow [0, 23, 0, 18, 0]$

A substitution can be made by replacing a non-empty token with another non-empty token:

$[0, 23, 0, 14, 0, 18, 0] \rightarrow [0, 85, 0, 14, 0, 18, 0]$

\subsection{Few-hot Vector}
A few-hot vector is a binary vector dominated by zeros but may contain one or more '1' values. Few-hot vectors represent classification labels where multiple classes are equally valid.

\subsection{Replacement Policy}
Given two sequences of tokens, an initial sequence, and a target sequences the replacement policy can identify all edits (insertions, deletions, and substitutions) that, when applied to the initial sequence, reduce the Levenshtein distance between the initial sequence and the target sequence.

When given an initial sequence and target sequence in normal form the replacement policy returns a few-hot vector for every token in the initial sequence. These few-hot vectors have the same length as the vocabulary. Together these few-hot vectors make up a $T$ by $V$ matrix where $T$ is the length of the initial sequence and $V$ is the length of the vocabulary. The few-hot matrix $F$ has elements $F_{t,v} = 1$ if setting the token at position $t$ in the initial sequence to the token with ID $v$ causes an edit that reduces the Levenshtein distance between the initial sequence and the target sequence ignoring empty tokens. $F_{t, v} = 1$ also if $v$ is the token ID of the token at position $t$ and there are no possible replacement tokens for position $t$ that would reduce the Levenshtein distance. Otherwise $F_{t,v} = 0$.

\section{Replacement Language Modeling}

Language modeling tasks involve predicting a probability distribution over the vocabulary for every token in the input sequence. In masked language modeling the loss is calculated between the predicted probabilities at the masked positions and a one-hot vector encoding the true token at the masked position. In causal language modeling the loss is calculated between the predicted probabilities at the last position and a one-hot vector encoding the true token at the next position.

I propose replacement language modeling which calculates the loss between the predicted probabilities at every position and a few-hot vector produced by the replacement policy. To accomplish this I suggest starting with a token sequence from a text dataset and performing edits on the sequence to corrupt it. Apply the replacement policy to the corrupted sequence and the original sequence to produce a few-hot vector for every token in the sequence. These vectors encode edits that reduce the Levenshtein distance. Next, feed the corrupted sequence into a language model and calculate the loss between the predicted probabilities at every position and the few-hot vectors produced by the replacement policy.

\noindent \textbf{Example}
\begin{enumerate}
    \item{Begin with a sequence from the training data: “I cooked dinner for my family”}
    \item{Corrupt the sequence by applying insertions, deletions, and substitutions: “I cooked dinner for my family” $\rightarrow$ “I dinner for” $\rightarrow$ “I car dinner for”}
    \item{Add the empty token before and after every token: “[EMT] I [EMT] car [EMT] dinner [EMT] for [EMT]”}
    \item{Use the Replacement Policy to identify what replacements that reduce the Levenshtein distance to the original sequence ignorign empty tokens}
    \begin{itemize}
            \item{[EMT] = { [EMT] = 1, 0 elsewhere }}
            \item{* I = { I = 1, 0 elsewhere }}
            \item{[EMT] = { [EMT] = 1, 0 elsewhere }}
            \item{car = { cooked = 1, 0 elsewhere }}
            \item{[EMT] = { [EMT] 1, 0 elsewhere }}
            \item{dinner = { dinner = 1, 0 elsewhere }}
            \item{[EMT] = { [EMT] = 1, 0 elsewhere }}
            \item{for = { for = 1, 0 elsewhere }}
            \item{[EMT] = { my = 1, family = 1, 0 elsewhere }}
    \end{itemize}

    \item{Use a language model to predict a probability distribution over the vocabulary at every time step}
    \item{Update the language model to optimize the negative log loss between the predicted probabilities and the few-hot vector}
\end{enumerate}
        
This training method can be viewed as behavior cloning of the Replacement Policy, except the language model does not know the target sequence

\end{document}
