{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from policy import find_actions, levenshtein_distance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign [unused0] at index 1 to [EMT] with add_special_tokens\n",
    "print(\"initial tokenizer vocab size: \", len(tokenizer))\n",
    "# relpace [unused0] with [EMT] in the vocab\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['[EMT]']})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = [\"[EMT]\"]\n",
    "inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Tweets.csv\")\n",
    "texts = df[\"text\"].tolist()\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMT_ID = tokenizer.convert_tokens_to_ids(\"[EMT]\")\n",
    "\n",
    "def add_empty_ids(input_ids, empty_id):\n",
    "    # If there are consecutive non-empty tokens, add an empty token between them\n",
    "    # If there are consecutive empty tokens, remove all but one\n",
    "    # Start and end with an empty token\n",
    "\n",
    "    output_ids = [empty_id]\n",
    "\n",
    "    for token_id in input_ids:\n",
    "        if token_id != empty_id:\n",
    "            if output_ids[-1] != empty_id:\n",
    "                output_ids.append(empty_id)\n",
    "            output_ids.append(token_id)\n",
    "        else:\n",
    "            if output_ids[-1] == empty_id:\n",
    "                continue\n",
    "            else:\n",
    "                output_ids.append(token_id)\n",
    "\n",
    "    if output_ids[-1] != empty_id:\n",
    "        output_ids.append(empty_id)\n",
    "    \n",
    "    return output_ids\n",
    "\n",
    "\n",
    "def get_labels(x_ids, y_ids, vocab_size):\n",
    "    # takes empty tokenized (every other token is empty token) input and output sequences\n",
    "\n",
    "    actions = find_actions(x_ids[1:-1:2], y_ids[1:-1:2])\n",
    "\n",
    "    to_pos = lambda i: 2 * i + 1\n",
    "\n",
    "    labels = np.zeros((len(x_ids), vocab_size))\n",
    "    for action in actions:\n",
    "        a, i, t = action\n",
    "        if a == \"R\":\n",
    "            labels[to_pos(i), t] = 1\n",
    "        elif a == \"I\":\n",
    "            labels[to_pos(i)-1, t] = 1\n",
    "        elif a == \"D\":\n",
    "            labels[to_pos(i), EMT_ID] = 1\n",
    "    \n",
    "    # add ones at the existing tokens where there is no action\n",
    "    non_action_mask = np.where(labels.sum(axis=1) == 0)[0]\n",
    "    x_ids = np.array(x_ids)\n",
    "    labels[non_action_mask, x_ids[non_action_mask]] = 1\n",
    "    \n",
    "\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def empty_tokenized_to_str(sequence):\n",
    "    return tokenizer.decode(sequence[1:-1:2])\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sequence = np.random.choice(texts)\n",
    "target_sequence = np.random.choice(texts)\n",
    "\n",
    "print(\"INITIAL\", initial_sequence)\n",
    "print(\"TARGET\", target_sequence)\n",
    "\n",
    "initial_sequence = tokenizer(initial_sequence)[\"input_ids\"][1:-1]\n",
    "target_sequence = tokenizer(target_sequence)[\"input_ids\"][1:-1]\n",
    "\n",
    "distance = levenshtein_distance(initial_sequence, target_sequence)\n",
    "print(\"DISTANCE\", distance)\n",
    "# print(tokenizer.decode(initial_sequence))\n",
    "initial_sequence = add_empty_ids(initial_sequence, EMT_ID)\n",
    "target_sequence = add_empty_ids(target_sequence, EMT_ID)\n",
    "\n",
    "# print initial_sequence as string\n",
    "# print(tokenizer.decode(initial_sequence))\n",
    "# print(empty_tokenized_to_str(initial_sequence))\n",
    "\n",
    "for i in range(100):\n",
    "    labels = get_labels(initial_sequence, target_sequence, len(tokenizer))\n",
    "    labels += np.random.uniform(0, 0.1, labels.shape)\n",
    "    new_ids = np.argmax(labels, axis=1)\n",
    "    if not (new_ids == initial_sequence).all():\n",
    "        replacement_id = np.random.choice(np.where(new_ids != initial_sequence)[0])\n",
    "        initial_sequence[replacement_id] = new_ids[replacement_id]\n",
    "        initial_sequence = add_empty_ids(initial_sequence, EMT_ID)\n",
    "    print(i+1, empty_tokenized_to_str(initial_sequence))\n",
    "\n",
    "    if (initial_sequence == target_sequence):\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56ebf659c10c7cf077cf84174ae674a7cf1e290812ec5148ed1aecd842ae738a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
