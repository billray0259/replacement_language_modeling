{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bill/dev/replacement_language_modeling/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from lib import levenshtein_distance, normalize_sequence, optimal_replacement_policy, corrupt_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial tokenizer vocab size:  30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30523, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reassign [unused0] at index 1 to [EMT] with add_special_tokens\n",
    "print(\"initial tokenizer vocab size: \", len(tokenizer))\n",
    "# relpace [unused0] with [EMT] in the vocab\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['[EMT]']})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 30522,   102]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = [\"[EMT]\"]\n",
    "inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica What @dhepburn said.',\n",
       " \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\",\n",
       " '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse',\n",
       " \"@VirginAmerica and it's a really big bad thing about it\",\n",
       " \"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\",\n",
       " '@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)',\n",
       " '@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP',\n",
       " \"@virginamerica Well, I didn't…but NOW I DO! :-D\",\n",
       " \"@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Tweets.csv\")\n",
    "texts = df[\"text\"].tolist()\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMT_ID = tokenizer.convert_tokens_to_ids(\"[EMT]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def empty_tokenized_to_str(sequence):\n",
    "    return tokenizer.decode(sequence[1:-1:2])\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  @ southwestair on hold with airline 45 min and counting. service is terrible!\n",
      "corrupted:  southwestenneair on mir with airline min [unused30] counting. bois terrible 265\n"
     ]
    }
   ],
   "source": [
    "rand_text = np.random.choice(texts)\n",
    "tokens = normalize_sequence(tokenizer.encode(rand_text)[1:-1], EMT_ID)\n",
    "corrupted_tokens = corrupt_sequence(tokens, tokenizer.vocab_size, EMT_ID, 10)\n",
    "print(\"original: \", empty_tokenized_to_str(tokens))\n",
    "print(\"corrupted: \", empty_tokenized_to_str(corrupted_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sequence = np.random.choice(texts)\n",
    "target_sequence = np.random.choice(texts)\n",
    "\n",
    "print(\"INITIAL\", initial_sequence)\n",
    "print(\"TARGET\", target_sequence)\n",
    "\n",
    "initial_sequence = tokenizer(initial_sequence)[\"input_ids\"][1:-1]\n",
    "target_sequence = tokenizer(target_sequence)[\"input_ids\"][1:-1]\n",
    "\n",
    "distance = levenshtein_distance(initial_sequence, target_sequence)\n",
    "print(\"DISTANCE\", distance)\n",
    "# print(tokenizer.decode(initial_sequence))\n",
    "initial_sequence = normalize_sequence(initial_sequence, EMT_ID)\n",
    "target_sequence = normalize_sequence(target_sequence, EMT_ID)\n",
    "\n",
    "# print initial_sequence as string\n",
    "# print(tokenizer.decode(initial_sequence))\n",
    "# print(empty_tokenized_to_str(initial_sequence))\n",
    "\n",
    "for i in range(100):\n",
    "    labels = optimal_replacement_policy(initial_sequence, target_sequence, len(tokenizer), EMT_ID)\n",
    "    labels += np.random.uniform(0, 0.1, labels.shape)\n",
    "    new_ids = np.argmax(labels, axis=1)\n",
    "    if not (new_ids == initial_sequence).all():\n",
    "        replacement_id = np.random.choice(np.where(new_ids != initial_sequence)[0])\n",
    "        initial_sequence[replacement_id] = new_ids[replacement_id]\n",
    "        initial_sequence = normalize_sequence(initial_sequence, EMT_ID)\n",
    "    print(i+1, empty_tokenized_to_str(initial_sequence))\n",
    "\n",
    "    if (initial_sequence == target_sequence):\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56ebf659c10c7cf077cf84174ae674a7cf1e290812ec5148ed1aecd842ae738a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
