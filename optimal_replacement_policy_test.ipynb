{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bill/dev/replacement_language_modeling/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from lib import optimal_replacement_policy, levenshtein_distance, normalize_sequence, get_replacement_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_replacement_tokenizer(\"distilbert-base-uncased\")\n",
    "empty_id = tokenizer.convert_tokens_to_ids(\"[EMT]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica What @dhepburn said.',\n",
       " \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\",\n",
       " '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse',\n",
       " \"@VirginAmerica and it's a really big bad thing about it\",\n",
       " \"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\",\n",
       " '@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)',\n",
       " '@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP',\n",
       " \"@virginamerica Well, I didn't…but NOW I DO! :-D\",\n",
       " \"@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Tweets.csv\")\n",
    "text_samples = df[\"text\"].tolist()\n",
    "text_samples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL @AmericanAir I lost my (basket) ballbag on your plane\n",
      "TARGET @united Gate agent hooked me up with alternate flights. If you have a way to PREVENT the constant issues, that would rock.\n",
      "DISTANCE 25\n",
      "[ 1.  1. 12. 13. 12. 13. 12. 13. 12. 13. 12. 13. 12. 13. 12. 13. 12. 13.\n",
      " 12. 13. 12. 13. 12. 13. 12. 13. 12. 13. 12.]\n",
      "1 @ americanair i lost my way basket ) ballbag on your plane\n",
      "[ 1.  1.  9. 10.  9. 10.  9. 10.  9. 10.  9. 10.  9.  1.  3.  4.  3.  4.\n",
      "  3.  4.  3.  4.  3.  4.  3.  4.  3.  4.  3.]\n",
      "2 @ americanair i lost my have way basket ) ballbag on your plane\n",
      "[1. 1. 7. 8. 7. 8. 7. 8. 7. 8. 7. 8. 7. 1. 1. 1. 3. 4. 3. 4. 3. 4. 3. 4.\n",
      " 3. 4. 3. 4. 3. 4. 3.]\n",
      "3 @ meir i lost my have way basket ) ballbag on your plane\n",
      "[1. 1. 4. 1. 3. 4. 3. 4. 3. 4. 3. 4. 3. 1. 1. 1. 3. 4. 3. 4. 3. 4. 3. 4.\n",
      " 3. 4. 3. 4. 3. 4. 3.]\n",
      "4 @ meir i lost my have way basket constant ) ballbag on your plane\n",
      "[1. 1. 4. 1. 3. 4. 3. 4. 3. 4. 3. 4. 3. 1. 1. 1. 2. 3. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "5 @ meir i lost my have way basket constant ) ballbag would your plane\n",
      "[1. 1. 4. 1. 3. 4. 3. 4. 3. 4. 3. 4. 3. 1. 1. 1. 2. 3. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "6 @ united meir i lost my have way basket constant ) ballbag would your plane\n",
      "[1. 1. 1. 1. 3. 1. 3. 4. 3. 4. 3. 4. 3. 4. 3. 1. 1. 1. 2. 3. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "7 @ united meir i lost my have way basket constant ) ballbag would your.\n",
      "[1. 1. 1. 1. 3. 1. 3. 4. 3. 4. 3. 4. 3. 4. 3. 1. 1. 1. 2. 3. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "8 @ united me upir i lost my have way basket constant ) ballbag would your.\n",
      "[1. 1. 1. 1. 3. 1. 1. 1. 2. 3. 2. 3. 2. 3. 2. 3. 2. 1. 1. 1. 2. 3. 2. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "9 @ united hooked me upir i lost my have way basket constant ) ballbag would your.\n",
      "[1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 3. 2. 3. 2. 3. 2. 3. 2. 1. 1. 1. 2. 3.\n",
      " 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "10 @ united hooked me upir i lost my have way basket constant ) ballbag would rock.\n",
      "[1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 3. 2. 3. 2. 3. 2. 3. 2. 1. 1. 1. 2. 3.\n",
      " 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "11 @ united hooked me upir flights i lost my have way basket constant ) ballbag would rock.\n",
      "[1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "12 @ united hooked me upir flights i lost my have way to basket constant ) ballbag would rock.\n",
      "[1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "13 @ united hooked me upir flights i lost my have way to prevent basket constant ) ballbag would rock.\n",
      "[1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "14 @ united hooked me upir flights i lost my have a way to prevent basket constant ) ballbag would rock.\n",
      "[1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "15 @ united hooked me upir flights i lost my have a way to prevent basket constant ),bag would rock.\n",
      "[1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "16 @ united gate hooked me upir flights i lost my have a way to prevent basket constant ),bag would rock.\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1.]\n",
      "17 @ united gate hooked me upir flights i lost you have a way to prevent basket constant ),bag would rock.\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1.]\n",
      "18 @ united gate hooked me upir flights. lost you have a way to prevent basket constant ),bag would rock.\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1.]\n",
      "19 @ united gate hooked me upir alternate flights. lost you have a way to prevent basket constant ),bag would rock.\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "20 @ united gate agent hooked me upir alternate flights. lost you have a way to prevent basket constant ),bag would rock.\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1.]\n",
      "21 @ united gate agent hooked me upir alternate flights. if you have a way to prevent basket constant ),bag would rock.\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1.]\n",
      "22 @ united gate agent hooked me upir alternate flights. if you have a way to prevent basket constant ), that would rock.\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1.]\n",
      "23 @ united gate agent hooked me upir alternate flights. if you have a way to prevent the constant ), that would rock.\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1.]\n",
      "24 @ united gate agent hooked me upir alternate flights. if you have a way to prevent the constant issues, that would rock.\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1.]\n",
      "25 @ united gate agent hooked me up with alternate flights. if you have a way to prevent the constant issues, that would rock.\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "initial_sequence = np.random.choice(text_samples)\n",
    "target_sequence = np.random.choice(text_samples)\n",
    "\n",
    "print(\"INITIAL\", initial_sequence)\n",
    "print(\"TARGET\", target_sequence)\n",
    "\n",
    "initial_sequence = tokenizer(initial_sequence)[\"input_ids\"][1:-1]\n",
    "target_sequence = tokenizer(target_sequence)[\"input_ids\"][1:-1]\n",
    "\n",
    "distance = levenshtein_distance(initial_sequence, target_sequence)\n",
    "print(\"DISTANCE\", distance)\n",
    "# print(tokenizer.decode(initial_sequence))\n",
    "initial_sequence = normalize_sequence(initial_sequence, empty_id)\n",
    "target_sequence = normalize_sequence(target_sequence, empty_id)\n",
    "\n",
    "# print initial_sequence as string\n",
    "# print(tokenizer.decode(initial_sequence))\n",
    "# print(empty_tokenized_to_str(initial_sequence))\n",
    "\n",
    "for i in range(100):\n",
    "    labels = optimal_replacement_policy(initial_sequence, target_sequence, len(tokenizer), empty_id)\n",
    "    labels += np.random.uniform(0, 0.1, labels.shape)\n",
    "    new_ids = np.argmax(labels, axis=1)\n",
    "    if not (new_ids == initial_sequence).all():\n",
    "        replacement_id = np.random.choice(np.where(new_ids != initial_sequence)[0])\n",
    "        initial_sequence[replacement_id] = new_ids[replacement_id]\n",
    "        initial_sequence = normalize_sequence(initial_sequence, empty_id)\n",
    "    print(i+1, tokenizer.decode(initial_sequence[1:-1:2]))\n",
    "\n",
    "    if (initial_sequence == target_sequence):\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56ebf659c10c7cf077cf84174ae674a7cf1e290812ec5148ed1aecd842ae738a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
